---
jupyter:
  jupytext:
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.14.1
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```python
import json
import warnings
import numpy as np
import pandas as pd
import networkx as nx
from matplotlib import cm
import seaborn as sns
import matplotlib.pyplot as plt
from pyvis.network import Network
from itertools import combinations
from scipy.sparse import csr_matrix
from matplotlib.colors import rgb2hex
import community as community_louvain
from sklearn.preprocessing import minmax_scale
from statsmodels.stats.multitest import fdrcorrection
from statsmodels.stats.contingency_tables import StratifiedTable
```

```python
def df2SM(data, indexVars, colVars, val):
    """ Help function to generate memory efficient pivot table """
    indexGrp = data.groupby(indexVars).grouper
    indexIdx = indexGrp.group_info[0]
    colGroup = data.groupby(colVars).grouper
    colIdx = colGroup.group_info[0]
    dataSM = csr_matrix((data[val].values, (indexIdx, colIdx)),
                         shape=(indexGrp.ngroups, colGroup.ngroups))

    return pd.DataFrame.sparse.from_spmatrix(
        dataSM, index=list(indexGrp), columns=list(colGroup))
```

```python
def fastStatifiedTable(a1, a2, strata):
    counts = []
    for stratum in strata:
        count = np.bincount(2 * a1[stratum] +  a2[stratum], minlength=4).reshape(2,2)
        counts.append(count)
    return np.array(counts).swapaxes(0, 2)
```

```python
def getICD(code, ICDmap, mode=1):
    return ICDmap[code[:3]]
```

```python
with open('../0.getICDlookup/data/ICDmap.json', 'r') as fp:
    ICDmap = json.load(fp)
```

```python
df = pd.read_pickle('data/IPadmissions.pkl')
```

```python
df = df.loc[(df['patientAge'] > 50) & (df['diabetesType2Flag'])]
```

```python
df = df.loc[df['mostRecent'] & (df['Sex'] != 'Unknown')].copy()
keepCols = ([
    'NHSnumber', 'site', 'Age' ,'Sex',  'Ethnicity', 
    'IMD (quintile)', 'diabetesFlag', 'diabetesType2Flag'
])
ICDcols = ['primaryICD', 'secondaryICD']
df = df[keepCols + ICDcols]
```

```python
# Merge primary and secondary codes and drop duplicates 
df['allCodes'] = df[ICDcols].apply(
    lambda x: set(pd.Series((x[0],) + (x[1])).dropna()), axis=1)
df['allCodes'] = df['allCodes'].apply(lambda x: set([i.split()[0] for i in x]))

# Extract subchapter codes
df['metaCodes'] = df['allCodes'].apply(
    lambda x: set([getICD(i, ICDmap)['code'] for i in x]))
```

```python
mode = 'allCodes'
```

```python
# Extract all pairwise comorbidities
codePairs = (
    df[mode].apply(
        lambda x: [tuple(sorted(x)) for x in combinations(x, 2)])
    .explode()
    .dropna()
    .value_counts()
    .sort_values(ascending=False)
)
```

```python
# Convert ICD codes to long format (1 row per code)
allCodes = df[mode].explode().rename('ICD')
df = pd.merge(df.drop(ICDcols, axis=1), allCodes, left_index=True, right_index=True).dropna()
df['present'] = True # Bool denoting presence of comorbidity
```

```python
# Convert to sparse boolean matrix of ICD codes
x = df2SM(df, keepCols, ['ICD'], 'present')
```

```python
codeCols = x.columns
x[keepCols] = x.index.tolist()
x = x.set_index('NHSnumber')
```

```python
strata = ['Sex']
x['strata'] = x[strata].apply(tuple, axis=1)
```

```python
# Overestimation of risk ratios by odds ratios in trials and cohort studies: alternatives to logistic regression
```

```python
indices = []
for stratum in x['strata'].unique():
    indices.append(np.array(x['strata'] == stratum))
```

```python
allLinks = []
with warnings.catch_warnings():
    warnings.simplefilter(action='ignore', category=FutureWarning)
    warnings.simplefilter(action='ignore', category=RuntimeWarning)
    for i, ((c1, c2), count) in enumerate(codePairs.iteritems()):
        try:
            a1 = np.array(x[c1])
            a2 = np.array(x[c2])
        except KeyError:
            continue
        tables = fastStatifiedTable(a1, a2, indices)
        minObs = np.sum(tables, axis=2).min()
        k = StratifiedTable(tables)
        allLinks.append((
            (c1, c2), count, minObs, k.oddsratio_pooled, k.riskratio_pooled,
            k.test_equal_odds().pvalue,  k.test_null_odds().pvalue)
        )
        if i % 10000 == 0:
            print(i)
allLinks = pd.DataFrame(allLinks)
allLinks.columns = ['ICDpair', 'count', 'minObs', 'OR', 'RR', 'pEqual', 'pNull']
allLinks.loc[allLinks['pNull'].notna(), 'FDR'] = (
    fdrcorrection(allLinks.loc[allLinks['pNull'].notna(), 'pNull'])[1]
)
allLinks.to_pickle(f'data/allNetworkEdges-{mode}.pkl')
```

```python
def processLinks(links, stat='OR', minVal=1, alpha=0.01, minObs=1):
    assert stat in links.columns
    allNodes = set(links['ICDpair'].apply(pd.Series).melt()['value'].tolist())
    sigLinks = links.loc[
        (links['FDR'] < alpha) 
        & (links[stat] > minVal)
        & (links['minObs'] >= minObs)
    ]
    allEdges = sigLinks.apply(lambda x: (*x['ICDpair'], x[stat]), axis=1).tolist()
    return allNodes, allEdges


def getGraphCentality(G, alphaMin=0.5):
    assert 0 <= alphaMin < 1
    centrality = nx.betweenness_centrality(G, weight='weight')
    centrality = pd.Series(centrality).to_frame().rename({0: 'centrality'}, axis=1)
    centrality['alpha'] = minmax_scale(centrality['centrality'], (alphaMin, 1))
    return centrality
  
    
def getGraphDegree(G, size=50, scale=10):
    assert (size > 0) and (scale > 1)
    degree = pd.DataFrame(G.degree()).set_index(0).rename({1: 'degree'}, axis=1)
    degree['size'] = minmax_scale(degree['degree'], (size, size * scale))
    return degree


def getNodePartion(G, colours=None):
    if colours is None:
        colours = ([
            (34,136,51), (204,187,68), (238,102,119), 
            (170,51,119), (68,119,170), (102,204,238), 
            (187,187,187)
        ])
    otherColour = colours[-1]
    partitionColours = colours[:-1]
    partitions = community_louvain.best_partition(G)
    mainPartions = (
        pd.Series(partitions.values())
        .value_counts()
        .head(len(partitionColours)).index
    )
    mainPartions = dict(zip(mainPartions, partitionColours))
    for node, partition in partitions.items():
        if partition not in mainPartions:
            partitions[node] = otherColour
        else:
            partitions[node] = mainPartions[partition]
    partitions = pd.Series(partitions).to_frame().rename({0: 'nodeRGB'}, axis=1)
    return partitions


def getNodeSummary(G, alphaMin=0.5, size=50, scale=10):
    centrality = getGraphCentality(G, alphaMin)
    degree = getGraphDegree(G, size, scale)
    partitionRGB = getNodePartion(G)
    summary = pd.merge(centrality, degree, left_index=True, right_index=True)
    summary = pd.merge(summary, partitionRGB, left_index=True, right_index=True)
    return summary
```

```python
allLinks = pd.read_pickle(f'data/allNetworkEdges-{mode}.pkl')
```

```python
# Filter all links for only tose in the top diabetes associations
diabAsoc = pd.read_pickle('data/topDiabetesAssociations.pkl')['ICD'].unique()
allLinks = allLinks.loc[allLinks['ICDpair'].apply(lambda x: (x[0] in diabAsoc) or (x[1] in diabAsoc))]
```

```python
alpha = 0.01
allNodes, allEdges = processLinks(allLinks, stat='RR', minVal=1, alpha=alpha, minObs=50)
```

```python
print(f'Nodes: {len(allNodes)}, Edges {len(allEdges)}')
```

```python
G = nx.Graph()
G.add_nodes_from(allNodes)
G.add_weighted_edges_from(allEdges)
```

```python
nodeSummary = getNodeSummary(G, alphaMin=0.5)
# Convert RGB to [0, 1] scale
nodeSummary['nodeRGB'] = nodeSummary['nodeRGB'].apply(
    lambda x: (x[0] / 255, x[1] / 255, x[2] / 255))
```

```python
for node in G.nodes():
    G.nodes[node]['size'] = nodeSummary.loc[node, 'size']
    alpha = nodeSummary.loc[node, 'alpha']
    rgb = nodeSummary.loc[node, 'nodeRGB']
    G.nodes[node]['color'] = rgb2hex((*rgb, alpha), keep_alpha=True)
    G.nodes[node]['font'] = {'size': 200}
```

```python
allEdges = {edge: G.edges[edge]['weight'] for edge in S.edges()}
allEdges = pd.Series(allEdges).to_frame().rename({0: 'OR'}, axis=1)
allEdges['logOR'] = np.log(allEdges['OR'])
allEdges['scaled'] = minmax_scale(allEdges['logOR'], (0.1, 1))
# Truncate to 1 in case of rounding error
allEdges['scaled'] = allEdges['scaled'].apply(lambda x: x if x < 1 else 1)
allEdges = allEdges['scaled'].to_dict()
```

```python
for edge in G.edges():
    weight = G.edges[edge]['weight']
    G.edges[edge]['width'] = np.log(S.edges[edge]['weight'])
    G.edges[edge]['color'] = rgb2hex((0, 0, 0, allEdges[edge]), keep_alpha=True)
```

```python
minDegree = 1
remove = [x for x in G.nodes() if G.degree(x) < minDegree]
G.remove_nodes_from(remove)
```

```python
net = Network(height='800px', width='75%', notebook=True)
net.from_nx(S)
net.toggle_physics(True)
net.show_buttons(filter_=["physics"])
net.barnes_hut()
net.save_graph('example.html')
```

```python

```

```python

```
